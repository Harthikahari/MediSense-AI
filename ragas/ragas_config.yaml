# RAGAS Evaluation Configuration for MediSense-AI

# Dataset configuration
dataset:
  path: "ragas/testset.jsonl"
  format: "jsonl"

# Metrics to evaluate
metrics:
  # Retrieval metrics
  - name: "context_precision"
    weight: 0.25
  - name: "context_recall"
    weight: 0.25

  # Generation metrics
  - name: "faithfulness"
    weight: 0.20
  - name: "answer_relevancy"
    weight: 0.20

  # Overall metrics
  - name: "answer_correctness"
    weight: 0.10

# Evaluation parameters
evaluation:
  batch_size: 10
  timeout_seconds: 120
  retry_attempts: 3

# LLM configuration for evaluation
llm:
  provider: "mock"  # Use mock for testing, "anthropic" for production
  model: "claude-3-sonnet"
  temperature: 0.0

# Embeddings configuration
embeddings:
  model: "sentence-transformers/all-MiniLM-L6-v2"
  dimension: 384

# Results output
results:
  output_dir: "ragas/results"
  save_artifacts: true
  generate_report: true

# Thresholds for passing evaluation
thresholds:
  context_precision: 0.7
  context_recall: 0.7
  faithfulness: 0.8
  answer_relevancy: 0.75
  answer_correctness: 0.7
